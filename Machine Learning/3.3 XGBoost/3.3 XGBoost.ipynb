{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "# First XGBoost model for Pima Indians dataset\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.20%\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=\",\")\n",
    "# split data into X and y\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# split data into train and test sets\n",
    "seed = 6\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.94425\tval-mlogloss:1.96620\n",
      "[5]\ttrain-mlogloss:1.25619\tval-mlogloss:1.32990\n",
      "[10]\ttrain-mlogloss:0.92857\tval-mlogloss:1.03200\n",
      "[15]\ttrain-mlogloss:0.73408\tval-mlogloss:0.86522\n",
      "[20]\ttrain-mlogloss:0.61020\tval-mlogloss:0.75997\n",
      "[25]\ttrain-mlogloss:0.52063\tval-mlogloss:0.69629\n",
      "[30]\ttrain-mlogloss:0.46110\tval-mlogloss:0.65904\n",
      "[35]\ttrain-mlogloss:0.41362\tval-mlogloss:0.63005\n",
      "[40]\ttrain-mlogloss:0.37701\tval-mlogloss:0.61471\n",
      "[45]\ttrain-mlogloss:0.34784\tval-mlogloss:0.60462\n",
      "[50]\ttrain-mlogloss:0.32365\tval-mlogloss:0.59703\n",
      "[55]\ttrain-mlogloss:0.30420\tval-mlogloss:0.59299\n",
      "[60]\ttrain-mlogloss:0.28684\tval-mlogloss:0.59276\n",
      "[65]\ttrain-mlogloss:0.27280\tval-mlogloss:0.59157\n",
      "[70]\ttrain-mlogloss:0.25990\tval-mlogloss:0.58868\n",
      "[75]\ttrain-mlogloss:0.24951\tval-mlogloss:0.58786\n",
      "[80]\ttrain-mlogloss:0.23898\tval-mlogloss:0.58960\n",
      "[82]\ttrain-mlogloss:0.23530\tval-mlogloss:0.59072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ZJ\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xgboost\\core.py:160: UserWarning: [16:08:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"is_unbalance\", \"max_delat_step\", \"n_estimators\", \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.9397394136807817\n",
      "Precision: 0.9397394136807817\n",
      "Recall: 0.9397394136807817\n",
      "F1-score:  0.9397394136807818\n",
      "report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.96      0.95       398\n",
      "         1.0       0.93      0.89      0.91       216\n",
      "\n",
      "    accuracy                           0.94       614\n",
      "   macro avg       0.94      0.93      0.93       614\n",
      "weighted avg       0.94      0.94      0.94       614\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb #导入xgboost包\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2) #拆分训练集和验证集\n",
    "\n",
    "'''生成xgb结构数据'''\n",
    "import xgboost as xgb\n",
    "train_xgb = xgb.DMatrix(X_train, label=y_train)\n",
    "test_xgb = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "'''定义xgb模型参数'''\n",
    "params = {'booster': 'gbtree',# 基分类器的烈性\n",
    "#          'objective': 'multi:softmax',\n",
    "          'num_class': 9,#几分类\n",
    "         'objective': 'multi:softprob',\n",
    "        'verbosity':2, #打印消息的详细程度\n",
    "#         'objective':'binary:logistic',# 目标函数\n",
    "        'tree_method':'auto', # 这是xgb框架自带的训练方法，可选参数为[auto,exact,approx,hist,gpu_hist]\n",
    "#         'eval_metric': 'auc', # 评价指标\n",
    "#         'scale_pos_weight':[1, 3, 1], # 正样本的权重，在二分类模型中，如果两个分类的样本比例失衡，可以设置该参数，模型效果会更高。一般取值:负样本数/正样本数，一般模型里偏少的部分会认为是正样本。\n",
    "          \n",
    "        'max_depth': 6, #树的深度，默认为6，一般取值[3,10] 越大偏差越小，方差越大，需综合考虑时间及拟合性。\n",
    "          \n",
    "        'min_child_weight': 3, #分裂叶子节点中样本权重和的最小值，如果新分裂的节点的样本权重和小于min_child_weight则停止分裂，默认为1，取值范围[0,],当值越大时，越容易欠拟合，当值越小时，越容易过拟合。\n",
    "          \n",
    "        'gamma':0, #别名min_split_loss 制定节点分裂所需的最小损失很熟下降值,节点分裂时，只有损失函数的减小值大于等于gamma，节点才会分裂，gamma越大，算法越保守，取值范围为[0,] 【0,1,5】\n",
    "          \n",
    "        'subsample': 0.8, #训练每棵树时子采样比例，默认为1，一般在[0.5,1]之间，调节该参数可以防止过拟合。\n",
    "          \n",
    "        'colsample_bytree':0.7, #训练每棵树时，使用特征占全部特征的比例，默认为1，典型值为[0.5,1]，调节该参数可以防止过拟合\n",
    "\n",
    "        'alpha':1, #别名reg_alpha，L1正则化，在高维度的情况下，调节该参数可以加快算法的速度，增大该值将是模型更保守，一般我们做特征选择的时候会用L1正则项，\n",
    "          \n",
    "        'lambda': 2, #L2正则化，调节、、增大该参数可以减少过拟合，默认值为1\n",
    "          \n",
    "        'eta': 0.1, #别名learning_rate 学习率一般越小越好，只是耗时会更长\n",
    "          \n",
    "        'n_estimators':500, #基学习器的个数，越大越好，偏差越小，但耗时会增加\n",
    "          \n",
    "        'max_delat_step':2, #限制每棵树权重改变的最大步长，默认值为0，及没有约束，如果为正值，则这个算法更加保守，通常不需要设置该参数，但是当样本十分不平衡时，对逻辑回归有帮助。\n",
    "\n",
    "        'nthread': -1,# 有多少处理器可以使用，默认为1，-1表示没有限制。\n",
    "        'silent': 1, #默认为0，不输出中间过程，=1，输出中间过程\n",
    "        'seed' : 2023, #随机种子\n",
    "        'is_unbalance':True}\n",
    "\n",
    "'''训练模型'''\n",
    "model1 = xgb.train(params,train_xgb,evals=[(train_xgb,'train'),(test_xgb,'val')], num_boost_round=100,early_stopping_rounds=10,verbose_eval=5,xgb_model=None)\n",
    "\n",
    "\n",
    "'''查看训练集（测试机替换为test_xgb,y_test)各项指标'''\n",
    "y_score = model1.predict(train_xgb)\n",
    "y_pred = np.argmax(y_score,axis=1)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "# display(pd.DataFrame(confusion_matrix(y_train, y_pred),columns=['prediction negetive','prediction positive'], index=['condition negtive', 'condition positive']))\n",
    "# print 'AUC:',roc_auc_score(y_train, y_pred)\n",
    "print ('ACC:',accuracy_score(y_train, y_pred))\n",
    "print ('Precision:',precision_score(y_train, y_pred, average='micro'))\n",
    "print ('Recall:',recall_score(y_train, y_pred, average='micro'))\n",
    "print ('F1-score: ', f1_score(y_train, y_pred, average='micro'))\n",
    "print ('report:\\n',classification_report(y_train,y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
